open introduction.dmc

// ===================== MASTER CLASS IA =======================================

# Master Class IA

  ## Pour qui ?

  show info Toute personne intéressée par le sujet
  show info
    Pas de prérequis préalable mais mieux si:
      - Bases d'Algèbre Linéaire
      - Bases d'Analyse de Fonctions
      - Bases d'Algorithmie


  ## Pourquoi ?

  show info
    Invention majeure en informatique
    Secteur à fort potentiel d'investissement
  show info
    L'IA est présente partout et sa place dans la société ne va cesser d'augmenter
  show info
    Pour faire avancer le domaine plus rapidement, il y a un besoin de partage


  ## Quoi ?

  show info Introduction au Deep Learning
  show info
    Réseaux de Neurones Convolutifs
    Yolo & Transfert de Style
  show info
    Réseaux de Neurones Récurrents
    Embedding & Traduction
  show info
    Apprentissage par Réenforcement
    Deep Q Network & Améliorations
  show info
    Modèles génératifs
    Pokemon & Super Résolution

// ===================== IA ====================================================

# L'Intelligence Artificielle

  ## Définitions

  show info Il n'y a pas qu'une seule définition
  show info
    L'Intelligence Artificielle est la science dont le but est de faire faire par
    une machine des tâches que l'homme accomplit en utilisant son intelligence
  show info On parle plutôt d'Informatique Heuristique
  show quote from J.L.Laurière
    Etude des activités intellectuelles de l'homme pour lesquelles
    aucune méthode n'est à priori connue


  ## IA Faible vs IA Forte

    ### Faible
    show info Non-sensible, qui se concentre sur une tâche précise et prédéfinie

    ### Forte
    show info
      Capable de répondre à tout problème, non seulement de produire un comportement
      intelligent mais aussi d'éprouver une impression d'une réelle conscience
      de soi

// ===================== MATHS FOR DEEP LEARNING ===============================

# Mathématiques pour le Deep Learning

  ## Algèbre Linéaire

    ### Matrices
      #### Définition
      show info Une Matrice est un tableau de dimension n par m
      show info Avec n = 3 et m = 2 on la Matrice M suivante:
      show maths M_{3,2}=\begin{bmatrix}a_{1,1}&a_{1,2}\\a_{2,1}&a_{2,2}\\a_{3,1}&a_{3,2}\end{bmatrix}

      #### Opérations
      show maths A=\begin{bmatrix}a_{1,1}&a_{1,2}\\a_{2,1}&a_{2,2}\\a_{3,1}&a_{3,2}\end{bmatrix}
      show maths B=\begin{bmatrix}b_{1,1}&b_{1,2}\\b_{2,1}&b_{2,2}\\b_{3,1}&b_{3,2}\end{bmatrix}

        ##### Addition
        show maths
          A+B=\begin{bmatrix}
            a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2}\\
            a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2}\\
            a_{3,1} + b_{3,1} & a_{3,2} + b_{3,2}
          \end{bmatrix}

        ##### Produit de Hadamard
        show maths
          A \circ B=\begin{bmatrix}
            a_{1,1} \times b_{1,1} & a_{1,2} \times b_{1,2}\\
            a_{2,1} \times b_{2,1} & a_{2,2} \times b_{2,2}\\
            a_{3,1} \times b_{3,1} & a_{3,2} \times b_{3,2}
          \end{bmatrix}

        ##### Multiplication Matricielle
        show maths A=\begin{bmatrix}a_{1,1}&a_{1,2}\\a_{2,1}&a_{2,2}\\a_{3,1}&a_{3,2}\end{bmatrix}
        show maths B=\begin{bmatrix}b_{1,1}&b_{1,2}\\b_{2,1}&b_{2,2}\\\end{bmatrix}
        show maths
          A.B=AB=\begin{bmatrix}
            a_{1,1}b_{1,1} + a_{1,2}b_{2,1} && a_{1,1}b_{1,2} + a_{1,2}b_{2,2}\\
            a_{2,1}b_{1,1} + a_{2,2}b_{2,1} && a_{2,1}b_{1,2} + a_{1,2}b_{2,2}
          \end{bmatrix}

        ##### Transposée
        show maths
          T(A)=A^{t}=\begin{bmatrix}
            a_{1,1} & a_{2,1} & a_{3,1}\\
            a_{1,2} & a_{2,2} & a_{3,2}
          \end{bmatrix}

    ### Généralisation
      show info
        Un Tenseur est une généralisation à n indices du concept de Matrice
        Une Matrice a 2 indices: 1 pour la ligne et 1 pour la colonne
        Un Vecteur a 1 indice
      show maths V=M_{3,1}=\begin{bmatrix}a_{1}\\a_{2}\\a_{3}\end{bmatrix}
      show maths M_{3,2}=\begin{bmatrix}a_{1,1}&a_{1,2}\\a_{2,1}&a_{2,2}\\a_{3,1}&a_{3,2}\end{bmatrix}


  ## Analyse de Fonctions

    ### Fonctions

      #### Une Variable
      show maths f(x) = x^{2}

      #### Plusieurs Variables
      show maths f(x, y) = (x - y)^{2}


    ### Dérivation

      #### Une Variable
      show maths f'(x) = 2x
      show maths (g \circ f)' = (g' \circ f) f'

      #### Plusieurs Variables
      show maths \frac{\partial f(x, y)}{\partial x} = 2x - 2y
      show maths \frac{\partial f(x, y)}{\partial y} = 2y - 2x

// ===================== PYTHON CHEAT SHEET ====================================

open cheat_sheet.py

# Python Cheat Sheet

  move to line 1
  ## Types
  show line 1 to 17
  show line 2 to 5
  show line 6 to 8
  show line 9 to 10
  show line 11 to 16
  show line 17

  move to line 19
  ## Containers
  show line 19 to 23
  show line 20
  show line 21
  show line 22
  show line 23

  move to line 25
  ## Variables
  show line 25 to 35
  show line 26 to 27
  show line 28
  show line 29
  show line 30 to 33
  show line 34
  show line 35

  move to line 37
  ## Convertions
  show line 37 to 45

  move to line 47
  ## Listes
  show line 47 to 63
  show line 48 to 49
  show line 50 to 57
  show line 58 to 63

  move to line 65
  ## Conditions
  show line 65 to 73

  move to line 75
  ## Boucles
  show line 75 to 81
  show line 76 to 78
  show line 80 to 81

  move to line 83
  ## Fonctions
  show line 83 to 88
  show line 84 to 86
  show line 88

  move to line 90
  ## Classes
  show line 90 to 112
  show line 91 to 104
  show line 106 to 107
  show line 109 to 110
  show line 112

  move to line 114
  ## Imports
  show line 114 to 117

  move to line 119
  ## Ecriture et Lecture de Fichier
  show line 119 to 127
  show line 120 to 123
  show line 125 to 127
  show line -1


open introduction.dmc

// ===================== DEEP LEARNING =========================================

# Introduction au Deep Learning

show info
  Le Deep Learning est l'apprentissage de Représentations
  Extracteur de caractéristique entraînable


    ## Machine Learning

      ### Régression Linéaire
      show maths y = W^{t}X
      show maths L(W, y_{i}, X_{i}) = \frac{1}{2} (y_{i}-W^{t}X_{i})^{2}
      show maths \frac{\partial L}{\partial W} = -(yi - W^{t}(t) X_{i}) X_{i}
      show maths W(t+1) = W(t) + \eta (t) (-\frac{\partial L}{\partial W})
      show info
        Minimiser la moyenne du coup
        Méthode du gradient stochastique
      show info
        Recherches dans les années 70 mais revenu au goût du jour
        Méthode imbattable en optimisation et seulement 3 lignes de code

      ### Perceptron
      show maths y = F(W^{t}X) \:\:\:\:\:\:\:\:\:\:\:\:\ F\,fonction\,seuil
      show maths L(W, y_{i}, X_{i}) = (F(W^{t}X_{i}) - y_{i}) W^{t} X_{i}
      show maths \frac{\partial L}{\partial W} = -(yi - F(W^{t}(t) X_{i})) X_{i}
      show maths W(t+1) = W(t) + \eta (t) (-\frac{\partial L}{\partial W})
      show info Régression Linéaire est un perceptron avec F = Id

      ### Régression Logistique
      show maths y = F(W^{t}X) \:\:\:\:\:\:\:\:\:\:\:\:\ F\,sigmoid
      show maths L(W, y_{i}, X_{i}) = 2log(1 + exp(y_{i}W^{t}X_{i}))
      show maths \frac{\partial L}{\partial W} = -(yi - F(W^{t}(t) X_{i})) X_{i}
      show maths W(t+1) = W(t) + \eta (t) (-\frac{\partial L}{\partial W})

    ## Limitations des Machines Linéaires
    show info
      La décision de surface est un Hyperplan de dimension N-1
      W est augmenté d'une dimension pour éviter de passer par l'origine
      Impossible de séparer certains ensembles par un hyperplan
    show quote from Thomas M. Cover
      A partir du moment où il y a plus de N points dans un espace à N dimensions,
      le probabilité de séparation devient de plus en plus faible


    ## Solutions

      show info Comment rendre un problème linéairement séparable ?

      ### Ruse
      show info Augmenter la dimension par une transformation non linéaire
      show info Exemple: Prendre le produit des entrées
      show maths
        (1, x_{1}, x_{2}): \, (1, x_{1}, x_{2}, x_{1}^{2}, x_{2}^{2}, x_{1}x_{2})
      show info
        La ligne de séparation devient conique
        Ne fonctionne pas à grande dimension, elle explose rapidement

      ### Autres Ruses
      show info Trop coûteux en terme de dimensionnalité

      ### Machines à Noyaux
      show info Machine à Vecteur de Support
      show maths F(X, W) = \sum_{k=1}^{P} W_{k} K(X, X_{k})
      show info
        La valeur est grande quand elle est proche et petite sinon
        Souvent un K est spécifique à un problème
        La méthode n'est donc pas généralisable

      ### Idées à retenir
      show info
        Les SVM sont performantes car le Noyau est surparamétrisé
        Cela rend le modèle assez flexible pour permettre l'apprentissage
        Il faudrait pénaliser le modèle pour pouvoir généraliser
      show quote from Vapnick - Chervonenkis
        La dimension de Vapnick d'un modèle de classification est le cardinal
        du plus grand ensemble pulvérisable par le modèle sans aucune erreur
      show maths
        Erreur + \sqrt{ \frac{ h(log(2N/h)+1) - log(\eta /4) }{ N } }
      show info Des features de meilleurs qualité entraînent de meilleurs résultats


    ## Deep Learning
    show info
      Combinaison de décisions élémentaires
      Les K sont appris par le modèle

      ### Aciennes Méthodes
      show info
        Prétraitement fixe
        Entraînement non supervisé pour augmenter la dimension de manière non linéaire et clairsemée
        Classifieur supervisé

      ### Nouvelles Méthodes
      show info Exactement la même sauf que chaque étape du processus est apprise
      show info
        Plus le modèle est profond plus les features sont de haut niveau
        Fonctionne car la nature est elle même compositionnelle
        C'est un Ensemble de concept divisés eux même en concepts plus simples
      show quote from Albert Einstein
        Ce qui est incompréhensible, c’est que le monde soit compréhensible.

      ### Challenges
      show info Apprendre des représentations du monde à plusieurs niveaux
      show info
        Exemple: Traitement visuel et audio chez les mamifères
        Ces traitements sont Feed-Forward
      show info Identifier les variables indépendantes explicatives du monde
      show info Exemple: Le Visage

      ### Différents Types
      show info
        Feed-Forward: MLP, CNN, RNN
        Feed-Backward: DéConv, Génératife
        Bi-Directionnel: Deep Boltzman Machines, Stacked Auto Encoder

      ### Différents Entraînement
        #### Supervisé
        #### Non Supervisé

      ### Réticences
      show info On peut approximer n'importe quelle fonction avec deux couches
      show info
        Oui mais pas efficace et trop coûteux
        Augmenter le nombre de couche diminue la complexité du problème
      show info
        Il est difficile de prouver la convergence des modèles de Deep Learning
        car les losses ne sont pas des fonctions convexes.
      show quote from Unknown
        - Que faites-vous, Monsieur?
        - Ben...  je cherche ma clef.
        - Mais êtes-vous sûr que vous l'avez perdue ici?
        - Non, absolument pas. Mais c'est le seul endroit où il y a de la lumière!
      show info
        L'apprentissage intéressant n'est pas convexe
        L'Humain n'apprend pas de manière convexe

    ## Exemple à la main

// ===================== NEXT ==================================================

# Prochains Cours ?
show info
  Etude d'un module, d'une topologie de réseau de neurones particulère

# The END
